# NLP Datasets Benchmark
Personnal repository that centralized various NLP datasets

## Datasets Description (took from https://github.com/huggingface)

|Dataset|Description|
|:---------|-------------|
|[ax](./GLUE/ax/)|This dataset evaluates sentence understanding through Natural Language Inference (NLI) problems. Use a model trained on MulitNLI to produce predictions for this dataset|
|[cola](./GLUE/cola/)|The Corpus of Linguistic Acceptability consists of sequence of words annotated with whether it is a grammatical English sentence|
|[mnli](./GLUE/mnli/)|The Multi-Genre Natural Language Inference Corpus is a crowdsourced collection of sentence pairs with textual entailment annotations. Given a premise sentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entailment), contradicts the hypothesis (contradiction), or neither (neutral).|
|[mnli_matched](./GLUE/mnli_matched/)|The matched validation and test splits from MNLI|
|[mnli_mismatched](./GLUE/mnli_mismatched/)|The mismatched validation and test splits from MNLI|
|[mrpc](./GLUE/mrpc/)|The Microsoft Research Paraphrase Corpus (Dolan & Brockett, 2005) is a corpus of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semantically equivalent.|
|[qnli](./GLUE/qnli/)|The Stanford Question Answering Dataset is a question-answering dataset consisting of question-paragraph pairs, where one of the sentences in the paragraph (drawn from Wikipedia) contains the answer to the corresponding question |
|[qqp](./GLUE/qqp/)|The Quora Question Pairs2 dataset is a collection of question pairs from the community question-answering website Quora. The task is to determine whether a pair of questions are semantically equivalent.|
|[rte](./GLUE/rte/)|The Recognizing Textual Entailment (RTE) datasets come from a series of annual textual entailment challenges|
|[sst2](./GLUE/sst2/)|The Stanford Sentiment Treebank consists of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence. It uses the two-way (positive/negative) class split, with only sentence-level labels|
|[stsb](./GLUE/stsb/)|The Semantic Textual Similarity Benchmark (Cer et al., 2017) is a collection of sentence pairs drawn from news headlines, video and image captions, and natural language inference data. Each pair is human-annotated with a similarity score from 1 to 5.|
|[wnli](./GLUE/wnli/)|The Winograd Schema Challenge (Levesque et al., 2011) is a reading comprehension task in which a system must read a sentence with a pronoun and select the referent of that pronoun from a list of choices
